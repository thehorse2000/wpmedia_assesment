# WP media crawler assesment 
##### Build specifically for WP media team.

## Problem presented
As an administrator, I want to see how my website web pages are linked to my home page so that I can manually search for ways to improve my SEO rankings.

## Solution
Develop a website for crawling the homepage and extracting all anchor links and display/save them to the user.

## Features
- User can Crawl any homepage url
- Crawl results are saved in a local json storage file
- A sitemap.xml file is generated according to the sitemap file protocols
- User can download the sitemap.xml file

## Tech stack
- PHP V8
- Laravel
- File system for storage
- Tailwind css for styling

#### Reasoning behind this tech stack
I decided to use laravel for easier project bootstrapping. 
I used the filesystem for storage for easier accessability, I wanted the evaluator to clone the project and be able to locally run it in minutes. But of course in a real-world app I would use a relational database.
I decided to use normal blade php in the frontend in order not to complicate the project. But I would be more comfortable using Vuejs or React

## How to run?
This is a laravel app, after cloning you should run ``npm install`` then ``npm run dev`` and ``php artisan serve`` to run the server.
There is no DB, so don't worry about any other configurations.

## Development
My first challenge in this project is that i didn't know much about sitemaps and how they should be generated. So the first thing i did is research what are sitemaps and how do they look, Once i learned enough i started scaffolding the app. I started with creating a new laravel instance and got straight to work (That is why i love laravel :) )

I decided not to use a crawler library for this project so i wrote my own crawling algorithm from scratch, Since the requirements of the crawler is not that advanced (We only fetch <a> tags).

**How i developed the crawler functionality:**
- I Created a Crawler model (in app/Models/Crawler.php) to handle this functionality.
- I used GuzzleHttp as an HTTP client liberary to make a request to the url and get a response
- I used DOMDocument to load HTML from the text response.
- I used DOMXPath to evaluate expressions and get all anchor tags in the HTML document, I learned some XPath expressions along the way.

**How i managed writing the sitemap.xml file:**
- I created a XMLWriter Model (in app/Models/XMLWriter.php) to handle the xml file writing to local storage(storage/app/public/sitemap.xml), I used the XMLwriter php class to acheive this.
- I followed the sitemap protocol in https://www.sitemaps.org/protocol.html
- There is only one sitemap.xml file that is generate at all times. Meaning that every new crawl deletes the old sitemap and generates a new one according to the results.

**How did i manage the storing of the results:**
- I created a CrawlerStorage model (in app/Models/CrawlerStorage.php) to handle storing of the results in local json file found in "storage/app/results.json". 

**How i manually calculated the sitemap priority field :**
Sitemap URL priority as I understand it, Is a range from 0.0 to 1.0 given to url according to its importance. 
Since there is no general standard on how to calculate it, And it wasn’t mentioned in the project requirements,  I created a little formula on my own, to calculate the priority according to how deeply nested the link page is. 
Giving the most deeply nested links a 0.0 and a superficial link such as "https;//whatever.com/index.html" a 1.0

## Project directory structure
If the evaluator isn't familiar with Laravel, i will guide you of where you should look for the code i have written
- **resources folder**: here you will find css and views, in views folder you will find all the FE blade php . I have used blade components for reusability (But i do regret not working with React :grimacing:)
- **app/http/controllers** is where the controllers for the Pages are.
- **app/Models**: as you guessed for Models. Models in this app are not linked to a database as you would normally see, i just used models to do the logical parts of writing XML and saving json to the results.json file. Probably not the best Laravel architecture design pattern but hey, it is logical and makes sense.
- **routes/web.php**: is where binding of routes to controllers or views occurs
- **storage/app**: this is where "sitemap.xml" file is generated and "results.json" is stored 
These are the most important folders, other folders are auto generated by the laravel framework


> _ I am open to any suggestions or improvements that should be done
